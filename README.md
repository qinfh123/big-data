# big data 大作业
  cookies用以知乎模拟登入储存cookies（还是没有搞清楚知乎汉字验证码怎么破，所以还是扫码）
  
  zhihu.py爬虫主题部分
  
  duzhihu是读知乎link的储存，用的是scrapy的框架，考虑到环境什么的不同，在zhihu.py中修改link为qq群中的txt文件（link文件夹）
  
  上传到github是因为zhihu.py还存在2个奇怪的bug，分别在问题的详细描述和评论的点赞数中。还有现在的储存方式写了json和xlsx的存储格式（不会mysql，失败了），
 然后字段还不确定爬哪些，所以zhihu.py还要修改
  
  如果出现奇奇怪怪的报错，可以把爬好的url在txt中删掉，然后重跑试试看（因为设置的time.sleep比较短，所以最好不要动自动运行的网页），我尝试了下昨天跑了2016.12.04的热榜
  ，没有报错，花了1小时多
  
  字段包括：排序	问题	问题描述	关注者	浏览数	问题图片链接	问题标签 回答正文	回答者	回答时间	点赞数	回答者链接 评论者	评论正文	评论者链接	点赞数	评论时间
可以见xlsx文件作为demo

